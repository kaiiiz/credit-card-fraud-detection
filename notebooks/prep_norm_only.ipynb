{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "077dff46-f112-458d-b92c-2f0ac634a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from statistics import mode\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "643628c1-28a9-4f38-8481-87622761415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d171995-1c05-431d-8ccc-3ff123239300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205d01a8a04f4acd933dd76fe8409dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe785a57fef404fa34335d47bc4de4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def down_sampling(raw_data):\n",
    "    df = raw_data.sort_values(['locdt', 'loctm'])\n",
    "    \n",
    "    # filter > 19 筆交易紀錄 for 卡號\n",
    "    fraud = df[df[\"fraud_ind\"] == 1]\n",
    "    non_fraud = df[df[\"fraud_ind\"] == 0]\n",
    "    card_tx_hist = {}\n",
    "\n",
    "    for idx, row in tqdm(non_fraud.iterrows()):\n",
    "        cano = row[\"cano\"]\n",
    "        if cano in card_tx_hist:\n",
    "            card_tx_hist[cano].append(idx)\n",
    "        else:\n",
    "            card_tx_hist[cano] = [idx]\n",
    "\n",
    "    tx_sample = []\n",
    "\n",
    "    for k, v in tqdm(card_tx_hist.items()):\n",
    "        if len(v) > 17:\n",
    "            card_tx_hist[k] = v[int(len(v) * 0.3) : ]\n",
    "        tx_sample += card_tx_hist[k]\n",
    "\n",
    "    sample_df = df.loc[tx_sample]\n",
    "    \n",
    "    conam_pct = df.loc[:,['cano','conam']].groupby('cano').mean()\n",
    "    kmeans = KMeans(n_clusters=5, max_iter=1000).fit(conam_pct)\n",
    "    conam_pct[\"cluster\"] = kmeans.labels_\n",
    "    \n",
    "    sampling_cards = pd.Series(conam_pct[conam_pct[\"cluster\"] == 0].index)\n",
    "    sampling_df_c1 = sample_df[sample_df[\"cano\"].isin(sampling_cards)].sample(frac=0.8)\n",
    "    \n",
    "    sampling_cards = pd.Series(conam_pct[conam_pct[\"cluster\"] == 1].index)\n",
    "    sampling_df_c2 = sample_df[sample_df[\"cano\"].isin(sampling_cards)].sample(frac=0.8)\n",
    "    \n",
    "    sampling_df_c = [sampling_df_c1, sampling_df_c2]\n",
    "    for i in range(2, 5):\n",
    "        sampling_df_c.append(sample_df[sample_df[\"cano\"].isin(pd.Series(conam_pct[conam_pct[\"cluster\"] == i].index))])\n",
    "    \n",
    "    return pd.concat((*sampling_df_c, fraud), 0)\n",
    "\n",
    "raw_data = down_sampling(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "227aa9cc-c580-469a-b988-c637640b98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acqic: dtype=int64, n_na=0\n",
      "       max=6884, min=0, mean=5994.93, std=1530.54, diversity=0.49%\n",
      "bacno: dtype=int64, n_na=0\n",
      "       max=163884, min=1, mean=82092.66, std=47337.04, diversity=8.31%\n",
      "cano: dtype=int64, n_na=0\n",
      "       max=213334, min=0, mean=108870.22, std=60988.91, diversity=11.26%\n",
      "conam: dtype=float64, n_na=0\n",
      "       max=7208.77, min=0.0, mean=658.0, std=412.02, diversity=5.45%\n",
      "contp: uni=[5 2 1 4 6 3 0]\n",
      "ecfg: uni=['N' 'Y']\n",
      "flg_3dsmk: uni=['N' 'Y' nan]\n",
      "hcefg: uni=[5 0 1 2 6 9 8 7 3]\n",
      "insfg: uni=['N' 'Y']\n",
      "mcc: dtype=int64, n_na=0\n",
      "       max=459, min=0, mean=298.48, std=78.09, diversity=0.04%\n",
      "mchno: dtype=int64, n_na=0\n",
      "       max=103307, min=0, mean=56004.14, std=30785.11, diversity=6.88%\n",
      "scity: dtype=int64, n_na=0\n",
      "       max=6671, min=0, mean=4751.29, std=1980.53, diversity=0.44%\n",
      "stocn: uni=[102  46  20  56 104  38  42  52  44  93  27  75  32   6  92  16  55  78\n",
      "  49  36  98  62   5  25  68  34   1  45  73  61   2  76  26  60  67 101\n",
      "  17  94  50  14  10  72  54  83  85  48  39 106   0  87  28  40  31  69\n",
      "  64   4 105 107  81  47  90  24  95  41  30  96  19   3  79 100  63  29\n",
      "  80  82  70  89   8  51  97  88  43  91  77  74   7  22  99  35  59  84\n",
      "   9  53  33  11  15  13  18  37]\n",
      "['acqic', 'bacno', 'cano', 'conam', 'mcc', 'mchno', 'scity'] ['contp', 'ecfg', 'flg_3dsmk', 'hcefg', 'insfg', 'stocn'] ['flg_3dsmk']\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(raw_data):\n",
    "    df = raw_data\n",
    "    \n",
    "    col_names_cont = [] # 數值型資料\n",
    "    col_names_disc = [] # 類別型資料\n",
    "    col_has_na = [] # 待補NA資料\n",
    "    for c in df.keys():\n",
    "        uni = df[c].unique()\n",
    "        n_na = pd.isna(df[c]).sum() # NA 數量\n",
    "        if n_na > 0:\n",
    "            col_has_na.append(c)       \n",
    "        if len(uni) < 200 :\n",
    "            print(f\"{c}: uni={uni}\")\n",
    "            col_names_disc.append(c)\n",
    "        else:\n",
    "            info = [ df[c].max(), df[c].min(), df[c].mean(), df[c].std()]\n",
    "            info = [ round(x,2) for x in info ]\n",
    "            offset = 1 if n_na else 0\n",
    "            diversity = (len(uni)-offset)/(len(df)-n_na)\n",
    "            print(f\"{c}: dtype={df[c].dtype}, n_na={n_na}\")\n",
    "            print(\"       max={}, min={}, mean={}, std={}, diversity={:.2f}%\".format(*info, diversity*100 ) )\n",
    "            if diversity == 1.0:\n",
    "                print(f\"       Delete col [{c}] due to diversity is 100% \")\n",
    "            else:\n",
    "                col_names_cont.append(c)\n",
    "\n",
    "    df = df.fillna(\"NA\")\n",
    "    \n",
    "    df_cont = df[col_names_cont].copy()\n",
    "    scaler = StandardScaler()\n",
    "    X_cont = scaler.fit_transform(df_cont)\n",
    "    df_cont = pd.DataFrame(data=X_cont, index=df.index, columns=col_names_cont)\n",
    "    \n",
    "    df_disc = df[col_names_disc].copy()\n",
    "    les = {}\n",
    "    for c in col_names_disc:\n",
    "        le = LabelEncoder()\n",
    "        df_disc.loc[:,c] = le.fit_transform(df_disc.loc[:,c])\n",
    "        les.update({c:le})\n",
    "        \n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    X_disc = ohe.fit_transform(df_disc)\n",
    "    # cut_point = ohe.feature_indices_\n",
    "    # print(\"feature cut point: \", cut_point)\n",
    "\n",
    "    new_col_names_disc = []\n",
    "    for c in col_names_disc: \n",
    "        le = les[c]\n",
    "        new_col_names_disc += [ c+'_'+str(cl) for cl in le.classes_ ]\n",
    "    assert len(new_col_names_disc) == X_disc.shape[1]\n",
    "\n",
    "    df_disc = pd.DataFrame(data=X_disc, index=df.index, columns=new_col_names_disc)\n",
    "\n",
    "    return pd.concat((df_cont, df_disc), 1), {\n",
    "        \"les\": les,\n",
    "        \"scaler\": scaler,\n",
    "        \"ohe\": ohe,\n",
    "    }\n",
    "\n",
    "important_cols = [\"acqic\", \"bacno\", \"cano\", \"conam\", \"contp\", \"ecfg\", \"flg_3dsmk\", \"hcefg\", \"insfg\", \"mcc\", \"mchno\", \"scity\", \"stocn\"]\n",
    "label = raw_data[\"fraud_ind\"]\n",
    "data, preprocessor = preprocessing(raw_data[important_cols])\n",
    "data[\"fraud_ind\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "64fc51f9-3a7e-4204-971c-1140b49a71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocessing(raw_data):\n",
    "    labels = raw_data[\"fraud_ind\"]\n",
    "    df = raw_data.drop([\"fraud_ind\", \"txkey\"], 1)\n",
    "    df = df.fillna(\"NA\")\n",
    "    \n",
    "    les = {}\n",
    "    for c in df.keys():\n",
    "        if not (df[c].dtype == np.int64 or df[c].dtype == np.float64):\n",
    "            le = LabelEncoder()\n",
    "            df.loc[:,c] = le.fit_transform(df.loc[:,c])\n",
    "            les.update({c:le})\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(df)\n",
    "    df = pd.DataFrame(data=x, index=df.index, columns=df.columns)\n",
    "    \n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    df = pd.DataFrame(min_max_scaler.fit_transform(df.values), index=df.index, columns=df.columns)\n",
    "    \n",
    "    df[\"fraud_ind\"] = labels\n",
    "    \n",
    "    return df, {\n",
    "        \"scaler\": scaler,\n",
    "        \"les\": les,\n",
    "        \"mms\": min_max_scaler,\n",
    "    }\n",
    "\n",
    "data, scaler = preprocessing(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8c5e02aa-4e32-41b1-ae49-11181c95c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/train_norm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bad59ef1-9626-4a01-ad79-ef274c4c9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "min_max_scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(min_max_scaler.fit_transform(data.values), columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22cfe23f-45b4-4ebd-810e-2bee50f6b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/preprocess_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "468ebe0c-91ed-49e3-9eb7-830e8d86df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor['mms'] = min_max_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b8670f65-0ec5-451e-8bbc-ee31d4e50aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../preprocessor.pkt\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
